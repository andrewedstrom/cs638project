{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ignore scikit learn deprication warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aparn\\Desktop\\cs638project\\learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#os.chdir('../blocking/')\n",
    "import pandas as pd\n",
    "import py_entitymatching as em\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 19)\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "# train = pd.read_csv('train_reduced.csv', encoding=\"ISO-8859-1\", index_col='_id')\n",
    "# test = pd.read_csv('test_reduced.csv', encoding=\"ISO-8859-1\", index_col='_id')\n",
    "from numpy import genfromtxt\n",
    "#myData = genfromtxt('train.csv',delimiter=',')\n",
    "\n",
    "#train = genfromtxt('train_reduced_CME_test.csv',delimiter=',')\n",
    "#test = genfromtxt('test_reduced_CME_test.csv',delimiter=',')\n",
    "\n",
    "train = genfromtxt('train_reduced.csv',delimiter=',')\n",
    "test = genfromtxt('test_reduced.csv',delimiter=',')\n",
    "\n",
    "print(train.shape)\n",
    "# test = test.dropna()\n",
    "# test.target\n",
    "train = train[~np.isnan(train).any(axis=1)]\n",
    "\n",
    "y = train[:,4] # label\n",
    "X = train[:,0:3] # data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# # TODO use F-1 for scoring, or a hybrid if possible\n",
    "# # we can use a for loop ['f1', 'accuracy', ]\n",
    "# # http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "# # logistic regression\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# logreg = LogisticRegression()\n",
    "# logreg.fit(X,y)\n",
    "# print('Logistic Regression:')\n",
    "# print(, cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "# # decision tree\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clf = DecisionTreeClassifier()\n",
    "# clf.fit(X,y)\n",
    "# print('Decision Tree:', cross_val_score(clf, X, y, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "# # random forest\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# forest = RandomForestClassifier(n_estimators=10, max_depth=10)\n",
    "# forest.fit(X,y)\n",
    "# print('Random Forest:', cross_val_score(forest, X, y, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "# # support vector machine\n",
    "# from sklearn.svm import LinearSVC\n",
    "# svm = LinearSVC()\n",
    "# svm.fit(X,y)\n",
    "# print('SVM:', cross_val_score(svm, X, y, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "# # naive bayes\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb = GaussianNB()\n",
    "# gnb.fit(X,y)\n",
    "# print('Naive Bayes:', cross_val_score(svm, X, y, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "# # KNN\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn = KNeighborsClassifier(n_neighbors=20)\n",
    "# knn.fit(X,y)\n",
    "# print('KNN:', cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = genfromtxt('train_reduced.csv',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 19)\n",
      "[  8.16009400e+06   1.96078431e-01   2.85714286e-01   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   4.63296703e-01   2.10000000e+01\n",
      "   1.60000000e-01  -8.00000000e+00   3.00000000e+00   1.00000000e+00\n",
      "   1.00000000e+00   0.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00   1.00000000e+00   0.00000000e+00]\n",
      "0.0\n",
      "y\n",
      "(124,)\n",
      "0.0\n",
      "X\n",
      "(124, 16)\n",
      "[  0.19607843   0.28571429   0.           0.           0.           0.4632967\n",
      "  21.           0.16        -8.           3.           1.           1.           0.\n",
      "   1.           1.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Awesome sauce.  Now let's fill in missing values rather than dropping them\n",
    "# reload train\n",
    "train = genfromtxt('train_reduced.csv',delimiter=',')\n",
    "\n",
    "# get rid of first row (I think this is just column labels)\n",
    "train = train[1:,:]\n",
    "\n",
    "# # # replace missing featureVals with 0.5 (halfway in between)\n",
    "# where_are_NaNs = np.isnan(train)\n",
    "# train[where_are_NaNs] = 0\n",
    "\n",
    "\n",
    "##Drop all nans:\n",
    "train = train[~np.isnan(train).any(axis=1)]\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(train[6,:])\n",
    "print\n",
    "print(train[6,train.shape[1]-1])\n",
    "len(train)\n",
    "\n",
    "y = train[:,train.shape[1]-1] # label\n",
    "X = train[:,1:train.shape[1]-2] # data\n",
    "print('y')\n",
    "print(y.shape)\n",
    "print(y[6])\n",
    "print('X')\n",
    "print(X.shape)\n",
    "print(X[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "precision: 0.866666666667\n",
      "recall: 0.675\n",
      "f1: 0.727380952381\n",
      "\n",
      "Decision Tree:\n",
      "precision: 0.941666666667\n",
      "recall: 0.841666666667\n",
      "f1: 0.879047619048\n",
      "\n",
      "Random Forest:\n",
      "precision: 0.941666666667\n",
      "recall: 0.866666666667\n",
      "f1: 0.828095238095\n",
      "\n",
      "LinearSVC:\n",
      "precision: 0.941666666667\n",
      "recall: 0.675\n",
      "f1: 0.695158730159\n",
      "\n",
      "SVC:\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f1: 0.0\n",
      "\n",
      "Naive Bayes:\n",
      "precision: 0.808333333333\n",
      "recall: 0.741666666667\n",
      "f1: 0.752619047619\n",
      "\n",
      "KNN:\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f1: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "models = [LogisticRegression(), \n",
    "          DecisionTreeClassifier(), \n",
    "          RandomForestClassifier(n_estimators=15, max_depth=15),\n",
    "          LinearSVC(),\n",
    "          SVC(),\n",
    "          GaussianNB(),\n",
    "          KNeighborsClassifier(n_neighbors=20)\n",
    "         ]\n",
    "names =  ['Logistic Regression', \n",
    "          'Decision Tree',\n",
    "          'Random Forest',\n",
    "          'LinearSVC',\n",
    "          'SVC',\n",
    "          'Naive Bayes',\n",
    "          'KNN']\n",
    "\n",
    "metrics = ['precision', 'recall', 'f1'] # 'accuracy', \n",
    "\n",
    "def compute_metrics(name, model):\n",
    "    model.fit(X,y)\n",
    "    print(\"{}:\".format(name))\n",
    "    for metric in metrics:\n",
    "        print(metric + ':', cross_val_score(model, X, y, cv=10, scoring=metric).mean())\n",
    "    print('')\n",
    "\n",
    "for i in range(len(names)):\n",
    "    compute_metrics(names[i], models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3566308243727598\n",
      "0.8513824884792627\n",
      "0.8722478238607271\n",
      "0.7929464423522148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# KNN tuning\n",
    "knn = KNeighborsClassifier()\n",
    "k_range = np.arange(1,50)\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='f1')\n",
    "grid.fit(X,y)\n",
    "print(grid.best_score_)\n",
    "\n",
    "\n",
    "# SVC tuning\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='f1')\n",
    "clf.fit(X,y)\n",
    "print(clf.best_score_)\n",
    "\n",
    "\n",
    "# random forest\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 5],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid,scoring='f1')\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "\n",
    "# Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\": [1, 2]\n",
    "             }\n",
    "\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"auto\",max_depth = None)\n",
    "\n",
    "ABC = AdaBoostClassifier(base_estimator = DTC)\n",
    "\n",
    "# run grid search\n",
    "grid_search_ABC = GridSearchCV(ABC, param_grid=param_grid, scoring = 'f1')\n",
    "grid_search_ABC.fit(X, y)\n",
    "print(grid_search_ABC.best_score_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
